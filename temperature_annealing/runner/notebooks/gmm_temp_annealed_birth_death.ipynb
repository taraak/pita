{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from src.models.components.mlp import MyMLP\n",
    "\n",
    "import torch\n",
    "import torchsde\n",
    "from torch import vmap\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "from src.energies.gmm_energy import GMM\n",
    "from fab.target_distributions import gmm\n",
    "\n",
    "from src.energies.base_prior import Prior, MeanFreePrior\n",
    "\n",
    "from src.models.components.clipper import Clipper\n",
    "\n",
    "from torch.func import hessian\n",
    "from math import sqrt\n",
    "from src.utils.data_utils import remove_mean\n",
    "from math import sqrt, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def product_of_gaussians(mu1, sigma1, mu2, sigma2, log_weights):\n",
    "    EPS = 1e-6\n",
    "\n",
    "    var1 = sigma1**2\n",
    "    var2 = sigma2**2\n",
    "\n",
    "    denom = var1 + var2\n",
    "    mu_prod = (mu1 * var2 + mu2 * var1) / denom\n",
    "    var_prod = (var1 * var2) / denom\n",
    "    std_prod = var_prod**0.5\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "\n",
    "    log_weights = (\n",
    "        log_weights\n",
    "        - 0.5 * torch.log(2 * np.pi * torch.prod(denom))\n",
    "        + torch.sum(-(diff**2) / (2 * denom), dim=-1)\n",
    "    )\n",
    "\n",
    "    return mu_prod, std_prod, log_weights\n",
    "\n",
    "\n",
    "def gmm_product(gmm1, gmm2):\n",
    "    means_1 = gmm1.gmm.locs\n",
    "    scale_trils_1 = gmm1.gmm.scale_trils\n",
    "    weights_1 = gmm1.gmm.cat_probs\n",
    "\n",
    "    means_2 = gmm2.gmm.locs\n",
    "    scale_trils_2 = gmm2.gmm.scale_trils\n",
    "    weights_2 = gmm2.gmm.cat_probs\n",
    "\n",
    "    K_1 = means_1.shape[0]\n",
    "    K_2 = means_2.shape[0]\n",
    "\n",
    "    new_weights = []\n",
    "    new_means = []\n",
    "    new_stds = []\n",
    "\n",
    "    for i in range(K_1):\n",
    "        for j in range(K_2):\n",
    "            mu1, sigma1 = means_1[i], torch.diagonal(scale_trils_1[i], dim1=-2, dim2=-1)\n",
    "            mu2, sigma2 = means_2[j], torch.diagonal(scale_trils_2[i], dim1=-2, dim2=-1)\n",
    "            log_weights1, log_weights2 = weights_1[i], weights_2[j]\n",
    "\n",
    "            # Product of two Gaussians\n",
    "            mu_prod, std_prod, z = product_of_gaussians(\n",
    "                mu1, sigma1, mu2, sigma2, log_weights1 + log_weights2\n",
    "            )\n",
    "\n",
    "            # New weight\n",
    "            new_weights.append(z)\n",
    "            new_means.append(mu_prod)\n",
    "            new_stds.append(std_prod)\n",
    "\n",
    "    # Stack results into tensors\n",
    "    new_weights = torch.stack(new_weights).to(gmm1.device)\n",
    "    new_means = torch.stack(new_means).to(gmm1.device)\n",
    "    new_stds = torch.stack(new_stds).to(gmm1.device)\n",
    "\n",
    "    # drop modes with small logprob\n",
    "    mask = torch.softmax(new_weights, dim=-1) > 1e-4\n",
    "    new_weights = new_weights[mask]\n",
    "    new_means = new_means[mask]\n",
    "    new_stds = new_stds[mask]\n",
    "\n",
    "    product_gmm = GMM(\n",
    "        dimensionality=gmm1.dimensionality,\n",
    "        n_mixes=new_means.shape[0],\n",
    "        loc_scaling=1.0,\n",
    "        log_var_scaling=1.0,\n",
    "        mean=new_means,\n",
    "        scale=new_stds,\n",
    "        cat_probs=new_weights,\n",
    "        device=device,\n",
    "        should_unnormalize=True,\n",
    "    )\n",
    "\n",
    "    return product_gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensionality = 2\n",
    "temperature = 1.0\n",
    "annealed_temperature = 1 / 3\n",
    "inverse_temperature = temperature / annealed_temperature  # beta in the paper\n",
    "\n",
    "target = GMM(\n",
    "    dimensionality=dimensionality,\n",
    "    n_mixes=40,\n",
    "    loc_scaling=40.0,\n",
    "    log_var_scaling=2.0,\n",
    "    device=device,\n",
    "    should_unnormalize=True,\n",
    ")\n",
    "\n",
    "annealed_target = gmm_product(target, gmm_product(target, target))\n",
    "# annealed_target = gmm_product(target, target)\n",
    "# target = target_temp2 #gmm_product(target_temp1, target_temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {}\n",
    "energies = {}\n",
    "choices = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annealed_target.gmm.locs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask= (torch.softmax(annealed_target.gmm.cat_probs, dim=-1)>1e-3)\n",
    "# masked_means = annealed_target.gmm.locs[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = target.sample_test_set(10000)\n",
    "b = annealed_target.sample_test_set(10000)\n",
    "\n",
    "plt.scatter(a[:, 0].detach().cpu(), a[:, 1].detach().cpu(), label=\"Original\", alpha=0.5, s=5)\n",
    "plt.scatter(b[:, 0].detach().cpu(), b[:, 1].detach().cpu(), label=\"Product\", alpha=0.5, s=5)\n",
    "plt.scatter(\n",
    "    annealed_target.gmm.locs[:, 0].cpu(), annealed_target.gmm.locs[:, 1].cpu(), label=\"Means\", s=20\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.get_dataset_fig(a, title=\"Temp 1\")\n",
    "plt.show()\n",
    "\n",
    "# title is annealed temperature up to 3 decimal points\n",
    "annealed_target.get_dataset_fig(b, title=f\"Temp {annealed_temperature:.3f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPSchedule:\n",
    "    def __init__(self, beta_min, beta_max):\n",
    "        self.beta_min = beta_min\n",
    "        self.beta_max = beta_max\n",
    "\n",
    "    def beta(self, t):\n",
    "        # beta = -2 * d/dt log_alpha\n",
    "        return self.beta_min + (self.beta_max - self.beta_min) * t\n",
    "\n",
    "    def log_mean_coeff(self, t):  # log_alpha\n",
    "        return -0.25 * t**2 * (self.beta_max - self.beta_min) - 0.5 * t * self.beta_min\n",
    "\n",
    "    # ddt log_mean_coeff = -0.5 * t * (beta_max - beta_min) - 0.5 * beta_min\n",
    "\n",
    "    def h(self, t):  # variance\n",
    "        return 1.0 - torch.exp(2.0 * self.log_mean_coeff(t))\n",
    "\n",
    "    def log_mean(self, t):\n",
    "        return torch.exp(self.log_mean_coeff(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_laplacian(model, target, t, xt, noise_schedule):\n",
    "    def func_wrap(t, xt):\n",
    "        return model(target, xt.unsqueeze(0), t.unsqueeze(0), noise_schedule).squeeze()\n",
    "\n",
    "    # Calculate the Hessian matrix of the model output with respect to the input\n",
    "    hessian_matrix = vmap(torch.func.jacrev(func_wrap, argnums=1))(t, xt)\n",
    "\n",
    "    # Calculate the Laplacian as the trace of the Hessian matrix\n",
    "    laplacian = hessian_matrix.diagonal(offset=0, dim1=-2, dim2=-1).sum(dim=-1)\n",
    "    return laplacian.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_Ut(target, x, t, noise_schedule, ve=True):\n",
    "    h_t = noise_schedule.h(t).to(device)\n",
    "    mean_coeff = None\n",
    "    if not ve:\n",
    "        mean_coeff = noise_schedule.log_mean(t).to(device)\n",
    "    target.gmm.convolve(\n",
    "        h_t * (target.data_normalization_factor**2), t, var_exploding=ve, mean_coeff=mean_coeff\n",
    "    )\n",
    "    energy = target(x)\n",
    "    target.gmm.reset()\n",
    "    return energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_nabla_Ut(target, x, t, noise_schedule, ve=True):\n",
    "    nabla_Ut = torch.func.jacrev(true_Ut, argnums=1)(target, x, t, noise_schedule, ve)\n",
    "    return nabla_Ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import qmc\n",
    "\n",
    "sampler = qmc.Sobol(d=1, scramble=False)\n",
    "\n",
    "\n",
    "def sample_cat(bs, next_u, logits):\n",
    "    # u, next_u = sample_uniform(bs, next_u)\n",
    "    u = sampler.random(bs).squeeze()\n",
    "    bins = torch.cumsum(torch.softmax(logits, dim=-1), dim=-1)\n",
    "    ids = np.digitize(u, bins.cpu())\n",
    "    ids = torch.tensor(ids, dtype=torch.long).to(device)\n",
    "    return ids, next_u\n",
    "\n",
    "\n",
    "def sample_cat_sys(bs, logits):\n",
    "    u = torch.rand(size=(1,))\n",
    "    u = (u + 1 / bs * torch.arange(bs)) % 1\n",
    "    bins = torch.cumsum(torch.softmax(logits, dim=-1), dim=-1)\n",
    "    ids = np.digitize(u, bins.cpu(), right=True)\n",
    "\n",
    "    ids[ids == logits.shape[-1]] = ids[ids == logits.shape[-1]] - 1\n",
    "    # print number of unique ids\n",
    "    # ids = torch.tensor(ids, dtype=torch.long).to(device)\n",
    "    # print(len(torch.unique(ids)))\n",
    "    return ids, None\n",
    "\n",
    "\n",
    "def sample_birth_death_clocks(\n",
    "    bs, accum_birth, accum_death, thresh_times, reset_transition_per_index=True\n",
    "):\n",
    "    # Generate random keys\n",
    "    death_mask = accum_death >= thresh_times\n",
    "    ids = torch.arange(bs).to(device)\n",
    "\n",
    "    # Sample candidate replacement indices according to accumulated birth weights\n",
    "    if reset_transition_per_index:\n",
    "        transition_probs = accum_birth / torch.sum(accum_birth, dim=-1, keepdim=True)\n",
    "        transition_probs = torch.nan_to_num(transition_probs, nan=0.0)\n",
    "\n",
    "        row_sums = torch.sum(transition_probs, dim=-1, keepdim=True)\n",
    "        zero_mask = row_sums == 0.0\n",
    "\n",
    "        # Replace zero-probability rows with uniform probabilities\n",
    "        uniform_probs = torch.ones_like(transition_probs) / transition_probs.size(-1)\n",
    "        transition_probs = torch.where(zero_mask, uniform_probs, transition_probs)\n",
    "        replace_ids = torch.vmap(lambda x: torch.multinomial(x, 1), randomness=\"different\")(\n",
    "            transition_probs\n",
    "        ).squeeze()\n",
    "    else:\n",
    "        transition_probs = accum_birth / torch.sum(accum_birth)\n",
    "        replace_ids = torch.multinomial(transition_probs, bs, replacement=True)\n",
    "\n",
    "    # Replace those entries chosen for killing\n",
    "    ids = torch.where(death_mask, replace_ids, ids)\n",
    "\n",
    "    # Sample new jump thresholds\n",
    "    new_thresh_times = torch.distributions.Exponential(1.0).sample((bs,)).to(device)\n",
    "\n",
    "    thresh_times = torch.where(death_mask, new_thresh_times, thresh_times)\n",
    "\n",
    "    # Reset birth and death weights in killed indices\n",
    "    if reset_transition_per_index:\n",
    "        accum_birth = torch.where(\n",
    "            death_mask.unsqueeze(-1), torch.zeros_like(accum_birth), accum_birth\n",
    "        )\n",
    "    else:\n",
    "        accum_birth = torch.where(death_mask, torch.zeros_like(accum_birth), accum_birth)\n",
    "\n",
    "    accum_death = torch.where(death_mask, torch.zeros_like(accum_death), accum_death)\n",
    "\n",
    "    metrics = (torch.sum(death_mask),)\n",
    "    return ids, accum_birth, accum_death, thresh_times, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VEReverse_coupled(torch.nn.Module):\n",
    "    noise_type = \"diagonal\"\n",
    "    sde_type = \"ito\"\n",
    "\n",
    "    def __init__(self, noise_schedule, inverse_temp=1.0, scale_diffusion=False):\n",
    "        super().__init__()\n",
    "        self.noise_schedule = noise_schedule\n",
    "        self.inverse_temp = inverse_temp\n",
    "        self.scale_diffusion = scale_diffusion\n",
    "\n",
    "    def f(self, t, x, resampling_interval=None):\n",
    "        if t.dim() == 0:\n",
    "            # repeat the same time for all points if we have a scalar time\n",
    "            t = t * torch.ones(x.shape[0]).to(x.device)\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            x.requires_grad_(True)\n",
    "            t.requires_grad_(True)\n",
    "\n",
    "            nabla_Ut = torch.vmap(true_nabla_Ut, in_dims=(None, 0, 0, None))(\n",
    "                target, x, t, self.noise_schedule\n",
    "            )\n",
    "            if self.scale_diffusion:\n",
    "                drift_X = (\n",
    "                    (self.inverse_temp + 1) / 2 * nabla_Ut * self.g(t, x).pow(2).unsqueeze(-1)\n",
    "                )\n",
    "            else:\n",
    "                drift_X = self.inverse_temp * nabla_Ut * self.g(t, x).pow(2).unsqueeze(-1)\n",
    "\n",
    "            drift_A = torch.zeros(x.shape[0]).to(x.device)\n",
    "\n",
    "            if resampling_interval is None:\n",
    "                return drift_X.detach(), drift_A.detach()\n",
    "\n",
    "            drift_A = 0.5 * (\n",
    "                (self.g(t, x)[:, None] * nabla_Ut).pow(2).sum(-1)\n",
    "                * (self.inverse_temp - 1)\n",
    "                * self.inverse_temp\n",
    "            )\n",
    "\n",
    "        return drift_X.detach(), drift_A.detach()\n",
    "\n",
    "    def g(self, t, x):\n",
    "        g = self.noise_schedule.g(t)\n",
    "        return g\n",
    "\n",
    "    def diffusion(self, t, x, diffusion_scale=1.0):\n",
    "        if t.dim() == 0:\n",
    "            # repeat the same time for all points if we have a scalar time\n",
    "            t = t * torch.ones_like(x).to(x.device)\n",
    "\n",
    "        if self.scale_diffusion:\n",
    "            return (\n",
    "                diffusion_scale\n",
    "                * self.g(t, x)\n",
    "                * torch.randn_like(x).to(x.device)\n",
    "                / np.sqrt(self.inverse_temp)\n",
    "            )\n",
    "        return diffusion_scale * self.g(t, x) * torch.randn_like(x).to(x.device)\n",
    "\n",
    "    def dlogq_dt(self, t, x, dx, dt):\n",
    "        if t.dim() == 0:\n",
    "            # repeat the same time for all points if we have a scalar time\n",
    "            t = t * torch.ones(x.shape[0]).to(x.device)\n",
    "        nabla_logq = torch.vmap(true_nabla_Ut, in_dims=(None, 0, 0, None))(\n",
    "            target, x, t, self.noise_schedule\n",
    "        )\n",
    "        return (nabla_logq * dx).sum(-1) - 0.5 * (self.g(t, x)[:, None] * nabla_logq).pow(2).sum(\n",
    "            -1\n",
    "        ) * dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler_maruyama_step_coupled(\n",
    "    sde,\n",
    "    x,\n",
    "    t,\n",
    "    a,\n",
    "    dt,\n",
    "    step,\n",
    "    resampling_interval,\n",
    "    accum_birth,\n",
    "    accum_death,\n",
    "    clock_thresholds,\n",
    "    logq,\n",
    "    resampling_strategy,\n",
    "    reset_transition_per_index,\n",
    "):\n",
    "    # Calculate drift and diffusion terms\n",
    "    drift_Xt, drift_At = sde.f(t, x, resampling_interval)\n",
    "    diffusion = sde.diffusion(t, x)\n",
    "    dx = drift_Xt * dt + diffusion * np.sqrt(dt)\n",
    "\n",
    "    logq_next = None\n",
    "    if logq is not None:\n",
    "        dlogq_dt = sde.dlogq_dt(t, x, dx, dt)\n",
    "        logq_next = logq + dlogq_dt\n",
    "\n",
    "    # Update the state\n",
    "    x_next = x + dx\n",
    "    a_next = a + drift_At * dt\n",
    "\n",
    "    if resampling_interval is None or step % resampling_interval != 0:  # or step<200:\n",
    "\n",
    "        # if step <200:\n",
    "        #     a_next = torch.zeros_like(a_next)\n",
    "        return x_next, a_next, logq_next, accum_birth, accum_death, clock_thresholds, x.shape[0]\n",
    "\n",
    "    if resampling_strategy == \"birth_death_clock\":\n",
    "        \"\"\"note, dw is correctly scaled by dt\"\"\"\n",
    "        dw = drift_At * dt\n",
    "        avg = torch.mean(dw, axis=0, keepdims=True)\n",
    "\n",
    "        \"\"\" accumulate thresholded weight increments (only sizing differences)\"\"\"\n",
    "        accum_death = accum_death + torch.where(dw - avg < 0, avg - dw, torch.zeros_like(dw))\n",
    "\n",
    "        bw_inc = torch.where(dw - avg > 0, dw - avg, torch.zeros_like(dw))\n",
    "        if reset_transition_per_index:\n",
    "            accum_birth = accum_birth + bw_inc[:, None]\n",
    "        else:\n",
    "            # accum_birth = accum_birth + bw_inc\n",
    "            accum_birth = torch.where(dw - avg > 0, dw - avg, torch.zeros_like(dw))\n",
    "\n",
    "        \"\"\" sampler resets accum_birth/death, clock_thresholds where necessary \"\"\"\n",
    "        choice, accum_birth, accum_death, clock_thresholds, metrics = sample_birth_death_clocks(\n",
    "            x.shape[0], accum_birth, accum_death, clock_thresholds\n",
    "        )\n",
    "        x_next = x_next[choice]\n",
    "        a_next = torch.zeros_like(a_next)\n",
    "        choice = choice.cpu().numpy()\n",
    "\n",
    "    elif resampling_strategy == \"systematic\":\n",
    "        # resample based on the weights\n",
    "        choice, _ = sample_cat_sys(x.shape[0], a_next)\n",
    "        a_next = torch.zeros_like(a_next)\n",
    "        x_next = x_next[choice]\n",
    "\n",
    "    return (\n",
    "        x_next,\n",
    "        a_next,\n",
    "        logq_next,\n",
    "        accum_birth,\n",
    "        accum_death,\n",
    "        clock_thresholds,\n",
    "        len(np.unique(choice)),\n",
    "    )\n",
    "\n",
    "\n",
    "def integrate_sde_coupled(\n",
    "    sde, x0, t_span, dt, resampling_interval, logq, resampling_strategy, reset_transition_per_index\n",
    "):\n",
    "    times = torch.arange(t_span[0], t_span[1], dt).to(device)\n",
    "    x = x0\n",
    "    x0.requires_grad = True\n",
    "    samples = []\n",
    "    logweights = []\n",
    "    logqs = []\n",
    "    choices = []\n",
    "    a = torch.zeros(x.shape[0]).to(device)\n",
    "\n",
    "    if reset_transition_per_index:\n",
    "        accum_birth = torch.zeros((a.shape[0], x.shape[0])).to(device)\n",
    "    else:\n",
    "        accum_birth = torch.zeros(a.shape[0]).to(device)\n",
    "    accum_death = torch.zeros(a.shape[0]).to(device)\n",
    "    clock_thresholds = torch.distributions.Exponential(1.0).sample((a.shape[0],)).to(device)\n",
    "    with torch.no_grad():\n",
    "        for step, t in enumerate(times):\n",
    "            x, a, logq, accum_birth, accum_death, clock_thresholds, choice = (\n",
    "                euler_maruyama_step_coupled(\n",
    "                    sde,\n",
    "                    x,\n",
    "                    1 - t,\n",
    "                    a,\n",
    "                    dt,\n",
    "                    step + 1,\n",
    "                    resampling_interval,\n",
    "                    accum_birth=accum_birth,\n",
    "                    accum_death=accum_death,\n",
    "                    clock_thresholds=clock_thresholds,\n",
    "                    logq=logq,\n",
    "                    resampling_strategy=resampling_strategy,\n",
    "                    reset_transition_per_index=reset_transition_per_index,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if target.is_molecule:\n",
    "                x = remove_mean(x, target.n_particles, target.n_spatial_dim)\n",
    "            samples.append(x)\n",
    "            logweights.append(a)\n",
    "            logqs.append(logq)\n",
    "            choices.append(choice)\n",
    "            # print(len(np.unique(choice.cpu().numpy())))\n",
    "\n",
    "    logqs = torch.stack(logqs) if logq is not None else None\n",
    "    return torch.stack(samples), torch.stack(logweights), logqs, np.array(choices)\n",
    "\n",
    "\n",
    "def generate_samples_weighted(\n",
    "    reverse_sde,\n",
    "    t_span=(0, 1),\n",
    "    num_integration_steps=100,\n",
    "    samples=None,\n",
    "    num_samples=200,\n",
    "    resampling_interval=None,\n",
    "    prior=None,\n",
    "    compute_logq=False,\n",
    "    resampling_strategy=\"systematic\",\n",
    "    reset_transition_per_index=False,\n",
    "):\n",
    "    if samples is None:\n",
    "        if prior is None:\n",
    "            raise ValueError(\"Either samples or prior distribution should be provided\")\n",
    "        samples = prior.sample(num_samples)\n",
    "\n",
    "    dt = 1 / num_integration_steps\n",
    "\n",
    "    samples, weights, logqs, choices = integrate_sde_coupled(\n",
    "        sde=reverse_sde,\n",
    "        x0=samples,\n",
    "        t_span=t_span,\n",
    "        dt=dt,\n",
    "        resampling_interval=resampling_interval,\n",
    "        logq=prior.log_prob(samples) if compute_logq else None,\n",
    "        resampling_strategy=resampling_strategy,\n",
    "        reset_transition_per_index=reset_transition_per_index,\n",
    "    )\n",
    "    if compute_logq:\n",
    "        return samples[-1], weights, logqs, choices\n",
    "    else:\n",
    "        return samples[-1], weights, choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Exploding SDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.components.noise_schedules import GeometricNoiseSchedule, QuadraticNoiseSchedule\n",
    "\n",
    "noise_schedule = GeometricNoiseSchedule(sigma_min=0.01, sigma_max=10 * sqrt(temperature))\n",
    "# noise_schedule = QuadraticNoiseSchedule(beta=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_int_steps = 1000\n",
    "num_samples = 10000\n",
    "num_seeds = 5\n",
    "\n",
    "prior = Prior(2, device=device, scale=noise_schedule.h(1) ** 0.5)\n",
    "annealed_prior = Prior(2, device=device, scale=(noise_schedule.h(1) / inverse_temperature) ** 0.5)\n",
    "\n",
    "\n",
    "prior_samples = prior.sample(num_samples)\n",
    "annealed_prior_samples = annealed_prior.sample(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Ito density Estimation\n",
    "ve_reverse_sde = VEReverse_coupled(noise_schedule, scale_diffusion=False)\n",
    "samples_hightemp, _, logq, _ = generate_samples_weighted(\n",
    "    ve_reverse_sde,\n",
    "    t_span=(0, 1),\n",
    "    num_integration_steps=num_int_steps,\n",
    "    resampling_interval=None,\n",
    "    num_samples=num_samples,\n",
    "    samples=prior_samples,\n",
    "    prior=prior,\n",
    "    compute_logq=True,\n",
    ")\n",
    "samples_energy = target(samples_hightemp)\n",
    "# target.get_dataset_fig(target.unnormalize(samples_hightemp), T=1.0, title=f\"Temperature {temperature}\")\n",
    "samples_hightemp_unnorm = target.unnormalize(samples_hightemp).detach().cpu().numpy()\n",
    "plt.scatter(\n",
    "    samples_hightemp_unnorm[:, 0],\n",
    "    samples_hightemp_unnorm[:, 1],\n",
    "    s=3,\n",
    "    label=f\"SDE T={temperature}\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "s = target.sample_test_set(1000).detach().cpu().numpy()\n",
    "plt.scatter(s[:, 0], s[:, 1], s=3, label=f\"True T={temperature}\", alpha=0.5)\n",
    "\n",
    "s = annealed_target.sample_test_set(1000).detach().cpu().numpy()\n",
    "plt.scatter(s[:, 0], s[:, 1], s=3, label=f\"True T={annealed_temperature}\", alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(samples_energy.detach().cpu(), bins=200)\n",
    "plt.hist((logq[-1].detach().cpu()) - log(target.data_normalization_factor**2), bins=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling without scaling diffusion\n",
    "ve_reverse_sde = VEReverse_coupled(\n",
    "    noise_schedule, inverse_temp=inverse_temperature, scale_diffusion=False\n",
    ")\n",
    "for strategy in [\"birth_death_clock\", \"systematic\"]:\n",
    "    # Resampling v1\n",
    "    s = []\n",
    "    c = []\n",
    "    for seed in range(num_seeds):\n",
    "        samples_resampled, log_weights, idxs = generate_samples_weighted(\n",
    "            ve_reverse_sde,\n",
    "            t_span=(0, 1),\n",
    "            samples=annealed_prior_samples,\n",
    "            num_integration_steps=num_int_steps,\n",
    "            resampling_interval=1,\n",
    "            resampling_strategy=strategy,\n",
    "        )\n",
    "        annealed_target.get_dataset_fig(\n",
    "            annealed_target.unnormalize(samples_resampled), title=\"resampled \" + strategy\n",
    "        )\n",
    "        plt.show()\n",
    "        # plt.plot(torch.linspace(1, 0, num_int_steps), log_weights.cpu().detach().numpy()[:, :20])\n",
    "        # plt.xlabel(\"Integration Time\")\n",
    "        # plt.ylabel(\"log-weights\")\n",
    "        # plt.show()\n",
    "\n",
    "        samples_resampled = samples_resampled.detach()\n",
    "        s.append(samples_resampled)\n",
    "        c.append(idxs)\n",
    "\n",
    "    samples[\"resampled \" + strategy] = torch.stack(s)\n",
    "    choices[\"resampled \" + strategy] = torch.tensor(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling with scaling diffusion\n",
    "ve_reverse_sde = VEReverse_coupled(\n",
    "    noise_schedule, inverse_temp=inverse_temperature, scale_diffusion=True\n",
    ")\n",
    "\n",
    "for strategy in [\"birth_death_clock\", \"systematic\"]:\n",
    "    s = []\n",
    "    c = []\n",
    "    for seed in range(num_seeds):\n",
    "        samples_resampled, log_weights, idxs = generate_samples_weighted(\n",
    "            ve_reverse_sde,\n",
    "            t_span=(0, 1),\n",
    "            samples=annealed_prior_samples,\n",
    "            num_integration_steps=num_int_steps,\n",
    "            resampling_interval=1,\n",
    "            resampling_strategy=strategy,\n",
    "        )\n",
    "        annealed_target.get_dataset_fig(\n",
    "            annealed_target.unnormalize(samples_resampled),\n",
    "            title=\"resampled scale_diffusion \" + strategy,\n",
    "        )\n",
    "        plt.show()\n",
    "        # plt.plot(torch.linspace(1, 0, num_int_steps), log_weights.cpu().detach().numpy()[:, :20])\n",
    "        # plt.xlabel(\"Integration Time\")\n",
    "        # plt.ylabel(\"dlogW\")\n",
    "        # plt.show()\n",
    "\n",
    "        samples_resampled = samples_resampled.detach()\n",
    "        s.append(samples_resampled)\n",
    "        c.append(idxs)\n",
    "\n",
    "    samples[\"resampled \" + strategy + \" scale_diffusion\"] = torch.stack(s)\n",
    "    choices[\"resampled \" + strategy + \" scale_diffusion\"] = torch.tensor(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(torch.linspace(1, 0, num_int_steps), choices[\"resampled systematic\"][0], label=\"Systematic seed 0\", alpha=0.5)\n",
    "# plt.plot(torch.linspace(1, 0, num_int_steps), choices[\"resampled systematic\"][1], label=\"Systematic seed 1\", alpha=0.5)\n",
    "# plt.plot(torch.linspace(1, 0, num_int_steps), choices[\"resampled birth_death_clock\"][0], label=\"Birth Death Clock seed 0\", alpha=0.5)\n",
    "# plt.plot(torch.linspace(1, 0, num_int_steps), choices[\"resampled birth_death_clock\"][1], label=\"Birth Death Clock seed 1\", alpha=0.5)\n",
    "\n",
    "\n",
    "# plt.plot(torch.linspace(1, 0, num_int_steps), choices[\"resampled systematic scale_diffusion\"][0], label=\"Systematic scale_diffusion seed 0\", alpha=0.5)\n",
    "# plt.plot(torch.linspace(1, 0, num_int_steps), choices[\"resampled systematic scale_diffusion\"][1], label=\"Systematic scale_diffusion seed 1\", alpha=0.5)\n",
    "\n",
    "# plt.plot(torch.linspace(1, 0, num_int_steps), choices[\"resampled birth_death_clock scale_diffusion\"][0], label=\"Birth Death Clock scale_diffusion seed 0\", alpha=0.5)\n",
    "# plt.plot(torch.linspace(1, 0, num_int_steps), choices[\"resampled birth_death_clock scale_diffusion\"][1], label=\"Birth Death Clock scale_diffusion seed 1\", alpha=0.5)\n",
    "\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Resampling without scaling diffusion\n",
    "s = []\n",
    "for seed in range(num_seeds):\n",
    "    ve_reverse_sde = VEReverse_coupled(\n",
    "        noise_schedule, inverse_temp=inverse_temperature, scale_diffusion=False\n",
    "    )\n",
    "    samples_not_resampled, log_weights, _ = generate_samples_weighted(\n",
    "        ve_reverse_sde,\n",
    "        t_span=(0, 1),\n",
    "        samples=annealed_prior_samples,\n",
    "        num_integration_steps=num_int_steps,\n",
    "        resampling_interval=num_int_steps + 1,\n",
    "    )  # num_int_steps+1\n",
    "    annealed_target.get_dataset_fig(\n",
    "        annealed_target.unnormalize(samples_not_resampled),\n",
    "        color=log_weights.cpu()[-1],\n",
    "        title=\"Not resampled \",\n",
    "    )\n",
    "    plt.show()\n",
    "    # plt.plot(torch.linspace(1, 0, num_int_steps), log_weights.cpu().detach().numpy()[:, :20])\n",
    "    # plt.xlabel(\"Integration Time\")\n",
    "    # plt.ylabel(\"log-weights\")\n",
    "    # plt.show()\n",
    "\n",
    "    samples_not_resampled = samples_not_resampled.detach()\n",
    "    s.append(samples_not_resampled)\n",
    "\n",
    "samples[\"not_resampled\"] = torch.stack(s)\n",
    "\n",
    "# # Generate samples at high temprature\n",
    "# samples_hightemp, _ = generate_samples_weighted(ve_reverse_sde, t_span=(0, 1), num_integration_steps=num_int_steps,\n",
    "#                                                 resampling_interval=None, num_samples=num_samples,\n",
    "#                                                 samples=prior_samples)\n",
    "# samples_energy = target(samples_hightemp)\n",
    "# target.get_dataset_fig(target.unnormalize(samples_hightemp), T=1.0, title=f\"Temperature {temperature}\")\n",
    "# # samples_hightemp_unnorm = target.unnormalize(samples_hightemp).detach().cpu().numpy()\n",
    "# # plt.scatter(samples_hightemp_unnorm[:, 0], samples_hightemp_unnorm[:, 1], s=3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Resampling and scaling diffusion\n",
    "s = []\n",
    "for seed in range(num_seeds):\n",
    "    ve_reverse_sde = VEReverse_coupled(\n",
    "        noise_schedule, inverse_temp=inverse_temperature, scale_diffusion=True\n",
    "    )\n",
    "    samples_not_resampled, log_weights_scaled, _ = generate_samples_weighted(\n",
    "        ve_reverse_sde,\n",
    "        t_span=(0, 1),\n",
    "        samples=annealed_prior_samples,\n",
    "        num_integration_steps=num_int_steps,\n",
    "        resampling_interval=num_int_steps + 1,\n",
    "    )  # num_int_steps+1\n",
    "    # samples_not_resampled = traj_samples[-1]\n",
    "    annealed_target.get_dataset_fig(\n",
    "        annealed_target.unnormalize(samples_not_resampled),\n",
    "        color=log_weights.cpu()[-1],\n",
    "        title=\"Not Resampled scale_diffusion\",\n",
    "    )\n",
    "    plt.show()\n",
    "    plt.plot(\n",
    "        torch.linspace(1, 0, num_int_steps), log_weights_scaled.cpu().detach().numpy()[:, :20]\n",
    "    )\n",
    "    plt.xlabel(\"Integration Time\")\n",
    "    plt.ylabel(\"log-weights\")\n",
    "    plt.show()\n",
    "\n",
    "    samples_not_resampled = samples_not_resampled.detach()\n",
    "    s.append(samples_not_resampled)\n",
    "\n",
    "samples[\"not_resampled scale_diffusion\"] = torch.stack(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.linspace(1, 0, num_int_steps), log_weights.mean(dim=1).cpu().detach().numpy())\n",
    "plt.plot(\n",
    "    torch.linspace(1, 0, num_int_steps), log_weights_scaled.mean(dim=1).cpu().detach().numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.linspace(1, 0, num_int_steps), log_weights.std(dim=1).cpu().detach().numpy())\n",
    "\n",
    "plt.plot(torch.linspace(1, 0, num_int_steps), log_weights_scaled.std(dim=1).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     start_idx = 1\n",
    "#     alpha_values = np.linspace(0.1, 1.0, start_idx)\n",
    "#     plt.plot(traj_samples_not_scaled[start_idx:, i, 0].detach().cpu(),\n",
    "#              traj_samples_not_scaled[start_idx:, i, 1].detach().cpu(), color='red', alpha=0.4)\n",
    "#     plt.scatter(traj_samples_not_scaled[start_idx:, i, 0].detach().cpu(),\n",
    "#                 traj_samples_not_scaled[start_idx:, i, 1].detach().cpu(),\n",
    "#                 s=5, alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#     start_idx = 1\n",
    "#     alpha_values = np.linspace(0.1, 1.0, start_idx)\n",
    "#     plt.plot(traj_samples[start_idx:, i, 0].detach().cpu(), traj_samples[start_idx:, i, 1].detach().cpu(), color='red', alpha=0.4)\n",
    "#     plt.scatter(traj_samples[start_idx:, i, 0].detach().cpu(), traj_samples[start_idx:, i, 1].detach().cpu(),\n",
    "#                 s=5, alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1000):\n",
    "#     if i%100==0 or i==999:\n",
    "#         plt.scatter(traj_samples[i][:, 0].detach().cpu(), traj_samples[i][:, 1].detach().cpu(),\n",
    "#                     s=3, alpha=0.5, c=log_weights[i].detach().cpu())\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(torch.linspace(1, 0, num_int_steps), log_weights.std(-1).cpu())\n",
    "# plt.xlabel(\"Integration Time\")\n",
    "# plt.ylabel(\"Standard Deviation of dlogW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.components.distribution_distances import eot\n",
    "from src.models.components.mmd import linear_mmd2, mix_rbf_mmd2, poly_mmd2\n",
    "from src.models.components.optimal_transport import wasserstein\n",
    "import ot as pot\n",
    "\n",
    "\n",
    "def compute_sample_based_metrics(a, b, a_energy, b_energy):\n",
    "    w1 = wasserstein(a.double(), b.double(), power=1)\n",
    "    w2 = wasserstein(a.double(), b.double(), power=2)\n",
    "    mmd_rbf = mix_rbf_mmd2(a, b, sigma_list=10 ** np.linspace(-2, 0, 10)).item()\n",
    "\n",
    "    H_b, x_b, y_b = np.histogram2d(b[:, 0].cpu().numpy(), b[:, 1].cpu().numpy(), bins=200)\n",
    "    H_a, x_a, y_a = np.histogram2d(a[:, 0].cpu().numpy(), a[:, 1].cpu().numpy(), bins=(x_b, y_b))\n",
    "    total_var = 0.5 * np.abs(H_a / H_a.sum() - H_b / H_b.sum()).sum()\n",
    "\n",
    "    energy_w2 = (\n",
    "        pot.emd2_1d(\n",
    "            a_energy,\n",
    "            b_energy,\n",
    "        )\n",
    "        .detach()\n",
    "        .cpu()\n",
    "        .item()\n",
    "    )\n",
    "    return w1, w2, mmd_rbf, total_var, energy_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_samples = target_temp1.sample_test_set(1000)\n",
    "# samples_resampled = samples_resampled.detach()\n",
    "# samples_not_resampled = samples_not_resampled.detach()\n",
    "\n",
    "# samples_resampled = target_temp1.unnormalize(samples_resampled)\n",
    "# samples_not_resampled = target_temp1.unnormalize(samples_not_resampled)\n",
    "\n",
    "# plt.scatter(samples_resampled[:, 0].cpu(), samples_resampled[:, 1].cpu(), s=3)\n",
    "# plt.scatter(test_samples[:, 0].cpu(), test_samples[:, 1].cpu(), s=3)\n",
    "# plt.scatter(samples_not_resampled[:, 0].cpu(), samples_not_resampled[:, 1].cpu(), s=3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = annealed_target.sample_test_set(num_samples)\n",
    "samples[\"Test\"] = annealed_target.normalize(test_samples).detach()\n",
    "\n",
    "\n",
    "for key in samples.keys():\n",
    "    e = []\n",
    "    for seed in range(num_seeds):\n",
    "        energy = annealed_target(samples[key][seed]).detach()\n",
    "        e.append(energy)\n",
    "    energies[key] = torch.stack(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = []\n",
    "metrics = [\"W1\", \"W2\", \"MMD\", \"Total Var\", \"Energy W2\"]\n",
    "for key in samples.keys():\n",
    "    if key == \"Test\":\n",
    "        continue\n",
    "    for seed in range(num_seeds):\n",
    "        data_df.append(\n",
    "            [\n",
    "                key,\n",
    "                *compute_sample_based_metrics(\n",
    "                    annealed_target.unnormalize(samples[key][seed]),\n",
    "                    annealed_target.unnormalize(samples[\"Test\"]),\n",
    "                    energies[key][seed],\n",
    "                    energies[\"Test\"],\n",
    "                ),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data_df)\n",
    "df.columns = [\"Method\"] + metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table processing\n",
    "def process_line(means, highlight, highlight_index, highlight_max, ignore_std):\n",
    "    if highlight:\n",
    "        if highlight_max:\n",
    "            tops = set(means.groupby(highlight_index).idxmax())\n",
    "        else:\n",
    "            tops = set(means.groupby(highlight_index).idxmin())\n",
    "    else:\n",
    "        tops = set()\n",
    "\n",
    "    def process_line(x):\n",
    "        if ignore_std:\n",
    "            if x.name in tops:\n",
    "                return rf\"\\textbf{{{x['mean']:0.3f}}}\"\n",
    "            return rf\"{x['mean']:0.3f}\"\n",
    "        if x.name in tops:\n",
    "            return rf\"\\textbf{{{x['mean']:0.3f} $\\pm$ {x['std']:0.3f}}}\"\n",
    "        return rf\"{x['mean']:0.3f} $\\pm$ {x['std']:0.3f}\"\n",
    "\n",
    "    return process_line\n",
    "\n",
    "\n",
    "def mean_pm_std(\n",
    "    data,\n",
    "    index,\n",
    "    columns,\n",
    "    value,\n",
    "    highlight=True,\n",
    "    highlight_cols=True,\n",
    "    highlight_max=True,\n",
    "    ignore_std=False,\n",
    "):\n",
    "    assert len(data) > 0\n",
    "    groupby = data.groupby([*index, *columns])\n",
    "    means = groupby.mean()[value].rename(\"mean\")\n",
    "    stds = groupby.std()[value].rename(\"std\")\n",
    "    ddf = pd.concat([means, stds], axis=1).T\n",
    "    highlight_index = columns if highlight_cols else index\n",
    "    ddf = ddf.apply(process_line(means, highlight, highlight_index, highlight_max, ignore_std))\n",
    "    ddf = ddf.reset_index().pivot(index=index, columns=columns)\n",
    "    ddf.columns = ddf.columns.droplevel(level=0)\n",
    "    return ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melt = df.melt(value_vars=metrics, id_vars=[\"Method\"], var_name=\"Metric\").reset_index()\n",
    "results = mean_pm_std(\n",
    "    df_melt, index=[\"Method\"], columns=[\"Metric\"], value=\"value\", highlight_max=False\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    results.to_latex(\n",
    "        float_format=\"{:.3f}\".format,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.components.utils import sample_from_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# axs[0].imshow(annealed_target.get_dataset_fig(annealed_target.unnormalize(s), title=keys));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fab.utils.plotting import plot_contours, plot_marginal_pair\n",
    "\n",
    "plotting_bounds = (-1.4 * 40, 1.4 * 40)\n",
    "\n",
    "\n",
    "def get_fig(target_dist, samples, ax, title, color, fp, cmap=None):\n",
    "    target_dist.gmm.to(\"cpu\")\n",
    "    plot_contours(\n",
    "        target_dist.gmm.log_prob,\n",
    "        bounds=plotting_bounds,\n",
    "        ax=ax,\n",
    "        n_contour_levels=50,\n",
    "        grid_width_n_points=200,\n",
    "    )\n",
    "\n",
    "    # plot dataset samples\n",
    "    plot_marginal_pair(samples, ax=ax, bounds=plotting_bounds, color=color, cmap=cmap)\n",
    "    ax.set_title(title, fontproperties=fp, fontsize=20)\n",
    "\n",
    "    ax.tick_params(labelsize=15)\n",
    "    ax.set_xticks(np.arange(-40, 60, 20))\n",
    "    for label in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "        label.set_fontproperties(fp)\n",
    "    ax.tick_params(labelsize=15)\n",
    "    target_dist.gmm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save samples and energies\n",
    "# torch.save(samples, \"figures/gmm_samples.pt\")\n",
    "# torch.save(energies, \"figures/gmm_energies.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the font\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "from tempfile import NamedTemporaryFile\n",
    "import urllib.request as urllib\n",
    "\n",
    "github_url = \"https://github.com/google/fonts/blob/main/apache/robotomono/RobotoMono%5Bwght%5D.ttf\"\n",
    "\n",
    "url = github_url + \"?raw=true\"  # You want the actual file, not some html\n",
    "\n",
    "response = urllib.urlopen(url)\n",
    "f = NamedTemporaryFile(delete=False, suffix=\".ttf\")\n",
    "f.write(response.read())\n",
    "f.close()\n",
    "prop = fm.FontProperties(fname=f.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 6, figsize=(30, 5), dpi=300)\n",
    "# plt.xticks(fontproperties=prop)\n",
    "# plt.yticks(fontproperties=prop)\n",
    "# plt.xticks(fontsize=10)\n",
    "# plt.yticks(fontsize=10)\n",
    "# for ax in axs:\n",
    "\n",
    "get_fig(\n",
    "    target, target.sample_test_set(4000), axs[0], \"Starting Disribution\", color=\"#3B347B\", fp=prop\n",
    ")\n",
    "\n",
    "titles = [\n",
    "    \"Ground Truth Samples\",\n",
    "    \"Target Score\",\n",
    "    \"Target Score + FKC\",\n",
    "    \"Tempered Noise\",\n",
    "    \"Tempered Noise + FKC\",\n",
    "]\n",
    "\n",
    "# colors = [\"Green\", \"Red\", \"Orange\", \"Purple\", \"Salmon\"]\n",
    "colors = [\"#AEADF0\", \"#1D9D79\", \"#D96002\", \"#756FB3\", \"#FFB900\"]\n",
    "\n",
    "for i, keys in enumerate(\n",
    "    [\n",
    "        \"Test\",\n",
    "        \"not_resampled\",\n",
    "        \"resampled systematic\",\n",
    "        \"not_resampled scale_diffusion\",\n",
    "        \"resampled systematic scale_diffusion\",\n",
    "    ]\n",
    "):\n",
    "    s = samples[keys].reshape(-1, 2)\n",
    "    s = sample_from_tensor(s, 2000)\n",
    "    # axs[i+1].imshow(annealed_target.get_dataset_fig(annealed_target.unnormalize(s), title=keys))\n",
    "    get_fig(\n",
    "        annealed_target,\n",
    "        annealed_target.unnormalize(s),\n",
    "        axs[i + 1],\n",
    "        title=titles[i],\n",
    "        color=colors[i],\n",
    "        fp=prop,\n",
    "    )\n",
    "\n",
    "    # change font for\n",
    "plt.savefig(f\"figures/gmm_samples.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby(\"Method\").mean()\n",
    "# # show mean and std\n",
    "# df.groupby(\"Method\").agg([np.mean, np.std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
