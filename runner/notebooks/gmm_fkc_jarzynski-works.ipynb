{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from math import log, sqrt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchsde\n",
    "from src.energies.base_prior import MeanFreePrior, Prior\n",
    "from src.energies.gmm_energy import GMM, GMMTempWrapper\n",
    "from src.models.components.clipper import Clipper\n",
    "from src.models.components.mlp import MyMLP, MyMLPTemperature\n",
    "from src.utils.data_utils import remove_mean\n",
    "from torch import vmap\n",
    "from torch.func import hessian\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchdiffeq import odeint\n",
    "from tqdm import tqdm\n",
    "\n",
    "from fab.target_distributions import gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensionality = 2\n",
    "temperature = 1.0\n",
    "annealed_temperature = 1 / 2\n",
    "inverse_temperature = temperature / annealed_temperature  # beta in the paper\n",
    "\n",
    "target = GMM(\n",
    "    dimensionality=dimensionality,\n",
    "    n_mixes=40,\n",
    "    loc_scaling=40.0,\n",
    "    log_var_scaling=2.0,\n",
    "    device=device,\n",
    "    should_unnormalize=True,\n",
    ")\n",
    "\n",
    "annealed_target = GMMTempWrapper(target, beta=int(inverse_temperature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = target.sample_test_set(1000)\n",
    "b = annealed_target.sample_test_set(1000)\n",
    "\n",
    "plt.scatter(a[:, 0].detach().cpu(), a[:, 1].detach().cpu(), label=\"Original\", alpha=0.5, s=5)\n",
    "plt.scatter(b[:, 0].detach().cpu(), b[:, 1].detach().cpu(), label=\"Product\", alpha=0.2, s=5)\n",
    "plt.scatter(\n",
    "    annealed_target.gmm.locs[:, 0].cpu(),\n",
    "    annealed_target.gmm.locs[:, 1].cpu(),\n",
    "    label=\"Means\",\n",
    "    s=20,\n",
    ")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = torch.stack([target.gmm.locs[10], target.gmm.locs[20], target.gmm.locs[18]])\n",
    "\n",
    "scales = torch.stack(\n",
    "    [target.gmm.scale_trils[10], target.gmm.scale_trils[20], target.gmm.scale_trils[18]]\n",
    ")\n",
    "\n",
    "\n",
    "mix = torch.distributions.Categorical(logits=torch.ones(3).to(device))\n",
    "com = torch.distributions.MultivariateNormal(modes, scale_tril=scales, validate_args=False)\n",
    "two_mode_dist = torch.distributions.MixtureSameFamily(\n",
    "    mixture_distribution=mix, component_distribution=com, validate_args=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_mode_samples = torch.randn(1000, 2).to(device) * target.gmm.scale_trils[10][\n",
    "    0, 0\n",
    "] + target.gmm.locs[10][None, :].repeat(1000, 1)\n",
    "init_samples = two_mode_dist.sample((1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.get_dataset_fig(a, title=\"Temp 1\")\n",
    "plt.scatter(\n",
    "    init_samples[:, 0].detach().cpu(),\n",
    "    init_samples[:, 1].detach().cpu(),\n",
    "    c=\"r\",\n",
    "    label=\"One mode\",\n",
    "    alpha=0.5,\n",
    "    s=5,\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# title is annealed temperature up to 3 decimal points\n",
    "annealed_target.get_dataset_fig(b, title=f\"Temp {annealed_temperature:.3f}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.components.utils import (\n",
    "    compute_laplacian_hutchinson,\n",
    "    rademacher,\n",
    "    sample_cat_sys,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_Ut(target, t, x, noise_schedule, ve=True):\n",
    "    h_t = noise_schedule.h(t).to(device)\n",
    "    mean_coeff = None\n",
    "    if not ve:\n",
    "        mean_coeff = noise_schedule.log_mean(t).to(device)\n",
    "    target.gmm.convolve(\n",
    "        h_t * (target.data_normalization_factor**2),\n",
    "        t,\n",
    "        var_exploding=ve,\n",
    "        mean_coeff=mean_coeff,\n",
    "    )\n",
    "    logp = target(x)\n",
    "    target.gmm.reset()\n",
    "    return -logp\n",
    "\n",
    "\n",
    "def true_nabla_Ut(target, t, x, noise_schedule, ve=True):\n",
    "    nabla_Ut = torch.func.jacrev(true_Ut, argnums=2)(target, t, x, noise_schedule, ve)\n",
    "    return nabla_Ut\n",
    "\n",
    "\n",
    "def true_dUt_dt(target, t, x, noise_schedule, ve=True):\n",
    "    dUt_dt = torch.func.jacrev(true_Ut, argnums=1)(target, t, x, noise_schedule, ve)\n",
    "    return dUt_dt\n",
    "\n",
    "\n",
    "# def compute_laplacian_true_exact(target, t, x, noise_schedule):\n",
    "#     def compute_hessian(target, t, x):\n",
    "#         return hessian(true_Ut, argnums=2)(target, t, x, noise_schedule)\n",
    "#     hessian_matrix = torch.vmap(compute_hessian, in_dims=(None, 0, 0))(target, t, x)\n",
    "#     laplacian = hessian_matrix.diagonal(offset=0, dim1=-2, dim2=-1).sum(dim=-1)\n",
    "#     return laplacian\n",
    "\n",
    "\n",
    "def compute_laplacian_true_exact(true_nabla_Ut, x):\n",
    "    N, d = x.shape\n",
    "    hessian_matrix = torch.zeros(N, d, d, device=x.device, dtype=x.dtype)\n",
    "\n",
    "    # Compute the Hessian row-wise\n",
    "    for i in range(d):  # ∂(∇U_t)_i / ∂x\n",
    "        grad2 = torch.autograd.grad(\n",
    "            true_nabla_Ut[:, i].sum(), x, retain_graph=True, create_graph=True\n",
    "        )[0]\n",
    "        hessian_matrix[:, i, :] = grad2  # i-th row\n",
    "    laplacian = hessian_matrix.diagonal(offset=0, dim1=-2, dim2=-1).sum(dim=-1)\n",
    "    return laplacian\n",
    "\n",
    "\n",
    "# def compute_laplacian_true_exact(target, t, xt, noise_schedule):\n",
    "#     def func_wrap(t, xt):\n",
    "#         return true_Ut(\n",
    "#             target, t.unsqueeze(0), xt.unsqueeze(0), noise_schedule\n",
    "#         ).squeeze()\n",
    "\n",
    "#     # Calculate the Hessian matrix of the model output with respect to the input\n",
    "#     hessian_matrix = vmap(hessian(func_wrap, argnums=1))(t, xt)\n",
    "\n",
    "#     # Calculate the Laplacian as the trace of the Hessian matrix\n",
    "#     laplacian = hessian_matrix.diagonal(offset=0, dim1=-2, dim2=-1).sum(dim=-1)\n",
    "#     return laplacian.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_laplacian_exact(model, t, xt, beta):\n",
    "    def func_wrap(t, xt):\n",
    "        return model(t.unsqueeze(0), xt.unsqueeze(0), beta).squeeze()\n",
    "\n",
    "    # Calculate the Hessian matrix of the model output with respect to the input\n",
    "    hessian_matrix = vmap(hessian(func_wrap, argnums=1))(t, xt)\n",
    "\n",
    "    # Calculate the Laplacian as the trace of the Hessian matrix\n",
    "    laplacian = hessian_matrix.diagonal(offset=0, dim1=-2, dim2=-1).sum(dim=-1)\n",
    "    return laplacian.detach()\n",
    "\n",
    "\n",
    "def compute_divergence(model, t, xt, beta):\n",
    "    def func_wrap(t, xt):\n",
    "        return model(t.unsqueeze(0), xt.unsqueeze(0), beta).squeeze()\n",
    "\n",
    "    # Calculate the Hessian matrix of the model output with respect to the input\n",
    "    nabla_bt = vmap(torch.func.jacrev(func_wrap, argnums=1))(t, xt)\n",
    "\n",
    "    # Calculate the Laplacian as the trace of the Hessian matrix\n",
    "    div_bt = nabla_bt.diagonal(offset=0, dim1=-2, dim2=-1).sum(dim=-1)\n",
    "    return div_bt.detach()\n",
    "\n",
    "\n",
    "def compute_laplacian(model, nabla_Ut, t, xt, beta, n_samples=1, exact=True):\n",
    "    if exact:\n",
    "        return compute_laplacian_exact(model, t, xt, beta)\n",
    "    else:\n",
    "        laplacian = 0\n",
    "        for _ in range(n_samples):\n",
    "            laplacian += compute_laplacian_hutchinson(nabla_Ut, t, xt)\n",
    "        return laplacian / n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler_maruyama_step(\n",
    "    sde: VEReverseSDE,\n",
    "    t: torch.Tensor,\n",
    "    x: torch.Tensor,\n",
    "    a: torch.tensor,\n",
    "    dt: float,\n",
    "    step: int,\n",
    "    batch_size: int,\n",
    "    diffusion_scale=1.0,\n",
    "    resampling_interval=-1,\n",
    "    inverse_temp=1.0,\n",
    "):\n",
    "    # Calculate drift and diffusion terms for num_eval_batches\n",
    "\n",
    "    drift_Xt = torch.zeros_like(x)\n",
    "    drift_At = torch.zeros_like(a)\n",
    "\n",
    "    for i in range(x.shape[0] // batch_size):\n",
    "        drift_Xt_i, drift_At_i = sde.f(\n",
    "            t,\n",
    "            x[i * batch_size : (i + 1) * batch_size],\n",
    "            inverse_temp,\n",
    "            resampling_interval,\n",
    "        )\n",
    "        drift_Xt[i * batch_size : (i + 1) * batch_size] = drift_Xt_i\n",
    "        drift_At[i * batch_size : (i + 1) * batch_size] = drift_At_i\n",
    "\n",
    "    # drift_Xt, drift_At = sde.f(t, x, resampling_interval)\n",
    "    drift_Xt = drift_Xt * dt\n",
    "    drift_At = drift_At * dt\n",
    "\n",
    "    if t.dim() == 0:\n",
    "        # repeat the same time for all points if we have a scalar time\n",
    "        t = t * torch.ones(x.shape[0]).to(x.device)\n",
    "    diffusion = (\n",
    "        diffusion_scale * sde.g(t)[:, None] * np.sqrt(dt) * torch.randn_like(x).to(x.device)\n",
    "    )\n",
    "\n",
    "    # Update the state\n",
    "    x_next = x + drift_Xt + diffusion\n",
    "    a_next = a + drift_At\n",
    "\n",
    "    if (resampling_interval == -1 or (step + 1) % resampling_interval != 0) or t[0] < 0.9:\n",
    "        return x_next, a_next\n",
    "\n",
    "    # resample based on the weights\n",
    "    choice, _ = sample_cat_sys(x.shape[0], a_next)\n",
    "    x_next = x_next[choice]\n",
    "    a_next = torch.zeros_like(a_next)\n",
    "\n",
    "    return x_next, a_next\n",
    "\n",
    "\n",
    "def integrate_sde_coupled(\n",
    "    sde,\n",
    "    x0,\n",
    "    t_span,\n",
    "    dt,\n",
    "    diffusion_scale,\n",
    "    resampling_interval,\n",
    "    inverse_temp,\n",
    "    resample_at_end,\n",
    "):\n",
    "    times = torch.arange(t_span[0], t_span[1], dt).to(device)\n",
    "    x = x0\n",
    "    x0.requires_grad = True\n",
    "    samples = []\n",
    "    logweights = []\n",
    "    a = torch.zeros(x.shape[0]).to(device)\n",
    "    with torch.no_grad():\n",
    "        for step, t in enumerate(times):\n",
    "            x, a = euler_maruyama_step(\n",
    "                sde,\n",
    "                1 - t,\n",
    "                x,\n",
    "                a,\n",
    "                dt,\n",
    "                step + 1,\n",
    "                batch_size=x.shape[0],\n",
    "                diffusion_scale=diffusion_scale,\n",
    "                resampling_interval=resampling_interval,\n",
    "                inverse_temp=inverse_temp,\n",
    "            )\n",
    "            samples.append(x)\n",
    "            logweights.append(a)\n",
    "\n",
    "        if resample_at_end:\n",
    "            # resample at the last step:\n",
    "            target_logprob = target(x)\n",
    "            if t.dim() == 0:\n",
    "                t = t * (torch.ones(x.shape[0])).to(x.device)\n",
    "            model_energy = sde.model.forward_energy(1 - t, x, inverse_temp)\n",
    "            logq_0 = -model_energy\n",
    "            a_next = target_logprob - (logq_0) + a\n",
    "            # a_next = torch.clamp(a_next, -10, 10)\n",
    "            choice, _ = sample_cat_sys(x.shape[0], a_next)\n",
    "            x = x[choice]\n",
    "            logweights[-1] = a_next\n",
    "            samples[-1] = x\n",
    "\n",
    "    return torch.stack(samples), torch.stack(logweights)\n",
    "\n",
    "\n",
    "def generate_samples_weighted(\n",
    "    reverse_sde,\n",
    "    prior,\n",
    "    num_samples=1000,\n",
    "    t_span=(0, 1),\n",
    "    num_integration_steps=1000,\n",
    "    samples=None,\n",
    "    diffusion_scale=1.0,\n",
    "    resampling_interval=-1,\n",
    "    inverse_temp=1,\n",
    "    resample_at_end=False,\n",
    "):\n",
    "\n",
    "    if samples is None:\n",
    "        samples = prior.sample(num_samples)\n",
    "\n",
    "    dt = 1 / num_integration_steps\n",
    "\n",
    "    samples, weights = integrate_sde_coupled(\n",
    "        sde=reverse_sde,\n",
    "        x0=samples,\n",
    "        t_span=t_span,\n",
    "        dt=dt,\n",
    "        resampling_interval=resampling_interval,\n",
    "        diffusion_scale=diffusion_scale,\n",
    "        inverse_temp=inverse_temp,\n",
    "        resample_at_end=resample_at_end,\n",
    "    )\n",
    "    return samples[-1], weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class EnergyNet(nn.Module):\n",
    "    def __init__(self, score_net: nn.Module, target, betas, noise_schedule, prior=None):\n",
    "        super(EnergyNet, self).__init__()\n",
    "        self.score_net = score_net\n",
    "        self.prior = prior\n",
    "        self.noise_schedule = noise_schedule\n",
    "        self.energy_function = {}\n",
    "        self.c = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    def forward_energy(self, t: torch.Tensor, x: torch.Tensor, beta) -> torch.Tensor:\n",
    "        # parametrize energy as <score_net(t, x), x>\n",
    "        # U_1 = self.prior.log_prob(x)\n",
    "        U_0 = -target(x) * beta\n",
    "        U_0 = torch.clamp(U_0, max=1e3, min=-1e3)\n",
    "\n",
    "        beta = beta * torch.ones(x.shape[0]).to(x.device)\n",
    "        beta = beta.unsqueeze(1)\n",
    "\n",
    "        h_t = self.noise_schedule.h(t).to(device)\n",
    "\n",
    "        c_s = 1 / (1 + h_t)  # 1 / (1 + sigma^2)\n",
    "        c_in = 1 / (1 + h_t) ** 0.5  # 1 / sqrt(1 + sigma^2)\n",
    "        c_out = h_t**0.5 * c_in  # sigma / sqrt(1 + sigma^2)\n",
    "        c_noise = (1 / 8) * torch.log(h_t)  # 1/4 ln(sigma)\n",
    "\n",
    "        def f_theta(t, xt, beta):\n",
    "            h_theta = self.score_net(t, xt, beta)\n",
    "            # h_theta = self.score_net(t, xt)\n",
    "            return torch.sum(h_theta * xt, dim=1)\n",
    "\n",
    "        U_theta = f_theta(c_noise, c_in[:, None] * x, beta)\n",
    "\n",
    "        E_theta = (1 - c_s) / (2 * h_t) * torch.linalg.norm(x, dim=-1) ** 2 - c_out / (\n",
    "            c_in * h_t\n",
    "        ) * U_theta\n",
    "        return E_theta\n",
    "\n",
    "        # return (1 - t) ** 3 * U_0 + t * E_theta\n",
    "\n",
    "    def forward(self, t: torch.Tensor, x: torch.Tensor, beta) -> torch.Tensor:\n",
    "        # beta = beta * torch.ones(x.shape[0]).to(x.device)\n",
    "        # beta = beta.unsqueeze(1)\n",
    "        # return self.score_net(t, x, beta)\n",
    "        U = self.forward_energy(t, x, beta)\n",
    "        nabla_U = torch.autograd.grad(U.sum(), x, create_graph=True)[0]\n",
    "        return nabla_U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_loss(energy_net, x, t, noise_schedule, beta):\n",
    "    h_t = noise_schedule.h(t.unsqueeze(1))\n",
    "    x.requires_grad = True\n",
    "\n",
    "    z = torch.randn_like(x)\n",
    "    xt = x + z * h_t**0.5\n",
    "\n",
    "    s_pred = -energy_net.forward(t, xt, beta)\n",
    "    s_true = -z\n",
    "    s_diff = torch.sum((s_pred * h_t**0.5 - s_true) ** 2, dim=(-1))  # * lambda_t\n",
    "    return s_diff.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def score_loss(energy_net, x, t, noise_schedule, beta):\n",
    "#     h_t = noise_schedule.h(t.unsqueeze(1))\n",
    "#     x.requires_grad = True\n",
    "\n",
    "#     z = torch.randn_like(x)\n",
    "#     xt = x + z * h_t**0.5\n",
    "\n",
    "#     s_pred = energy_net.forward(t, xt, beta)\n",
    "#     s_true = torch.vmap(true_nabla_Ut, in_dims=(None, 0, 0, None))(\n",
    "#         target, t, xt, noise_schedule\n",
    "#     )\n",
    "#     s_diff = torch.sum((s_pred - s_true) ** 2, dim=(-1))  # * lambda_t\n",
    "#     return s_diff.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.components.mlp import MyMLP\n",
    "from torchcfm.conditional_flow_matching import ConditionalFlowMatcher\n",
    "\n",
    "FM = ConditionalFlowMatcher(sigma=0.0)\n",
    "\n",
    "\n",
    "def cfm_loss(model, x1, t, beta):\n",
    "    h_t = noise_schedule.h(t.unsqueeze(1))\n",
    "    x0 = torch.randn_like(x1)\n",
    "    t, xt, ut = FM.sample_location_and_conditional_flow(x0, x1)\n",
    "\n",
    "    vt = model(t, xt, beta)\n",
    "    loss = torch.mean((vt - ut) ** 2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cfm_inference(model, xt, t, beta, dt):\n",
    "    with torch.no_grad():\n",
    "        vt = model(t * torch.ones(xt.shape[0]).to(device), xt, beta)\n",
    "        xt_new = xt + vt * dt\n",
    "        return xt_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples_ode(model, prior, beta, n_samples=1000, t_span=[0, 1], dt=0.001):\n",
    "    times = torch.arange(t_span[0], t_span[1], dt).to(device)\n",
    "    samples = []\n",
    "    x = prior.sample(n_samples)\n",
    "    with torch.no_grad():\n",
    "        for t in times:\n",
    "            x = cfm_inference(model, x, t, beta, 1e-3)\n",
    "            samples.append(x)\n",
    "    return torch.stack(samples)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.components.noise_schedules import (\n",
    "    ElucidatingNoiseSchedule,\n",
    "    GeometricNoiseSchedule,\n",
    ")\n",
    "\n",
    "# noise_schedule = GeometricNoiseSchedule(sigma_min=0.001, sigma_max=5)\n",
    "noise_schedule = ElucidatingNoiseSchedule(sigma_min=0.002, sigma_max=80, rho=7)\n",
    "\n",
    "# betas = [1, 2, 3, 4]\n",
    "betas = [1.0]  # torch.arange(1, 5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = {}\n",
    "for beta in betas:\n",
    "    priors[beta] = Prior(2, device=device, scale=(noise_schedule.h(1) / beta) ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.components.prioritised_replay_buffer import SimpleBuffer\n",
    "\n",
    "buffer = {}\n",
    "\n",
    "for beta in betas:\n",
    "    buffer[beta] = SimpleBuffer(\n",
    "        2,\n",
    "        10000,\n",
    "        1000,\n",
    "        initial_sampler=None,\n",
    "        device=device,\n",
    "        fill_buffer_during_init=False,\n",
    "        sample_with_replacement=True,\n",
    "        prioritize=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_epochs = 100\n",
    "inner_epochs = 500\n",
    "batch_size = 1024\n",
    "\n",
    "\n",
    "init_samples = target.sample_test_set(10000)\n",
    "init_samples_normalized = target.normalize(init_samples)\n",
    "init_samples_energy = target(init_samples_normalized)\n",
    "\n",
    "train_dataset = TensorDataset(init_samples_normalized.detach(), init_samples_energy.clone())\n",
    "trainloader = {}\n",
    "\n",
    "for beta in betas:\n",
    "    trainloader[beta] = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_net = MyMLPTemperature().to(device)\n",
    "# score_net = MyMLP().to(device)\n",
    "energy_net = EnergyNet(score_net, target=target, betas=betas, noise_schedule=noise_schedule).to(\n",
    "    device\n",
    ")\n",
    "optimizer = Adam(energy_net.parameters(), lr=1e-3)\n",
    "weighted = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VEReverseSDE(torch.nn.Module):\n",
    "    def __init__(self, energy_net, noise_schedule, score_model=None, gamma=1.0):\n",
    "        super().__init__()\n",
    "        self.energy_net = energy_net\n",
    "        self.score_model = score_model\n",
    "        self.noise_schedule = noise_schedule\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def f(self, t, x, beta, resampling_interval=-1):\n",
    "        if t.dim() == 0:\n",
    "            # repeat the same time for all points if we have a scalar time\n",
    "            t = t * torch.ones(x.shape[0]).to(x.device)\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            x.requires_grad_(True)\n",
    "            t.requires_grad_(True)\n",
    "\n",
    "            nabla_Ut = self.energy_net(t, x, beta)\n",
    "            # nabla_Ut = torch.vmap(true_nabla_Ut, in_dims=(None, 0, 0, None))(\n",
    "            #     target, t, x, self.noise_schedule\n",
    "            # )\n",
    "\n",
    "            if self.score_model is not None:\n",
    "                model_out = self.score_model(t, x, beta)\n",
    "                bt = -model_out * self.g(t).pow(2).unsqueeze(-1) / 2\n",
    "            else:\n",
    "                bt = -nabla_Ut * self.g(t).pow(2).unsqueeze(-1) / 2\n",
    "\n",
    "            drift_X = -nabla_Ut * self.g(t).pow(2).unsqueeze(-1) / 2 + bt\n",
    "\n",
    "            if int(t[0] * 1000) % 100 == 0:\n",
    "                print(\n",
    "                    \"time: \",\n",
    "                    t[0],\n",
    "                    (torch.linalg.norm(model_out - nabla_Ut) ** 2).mean().item(),\n",
    "                )\n",
    "            # print(\"nabla_Ut\", nabla_Ut[:3])\n",
    "            # print(\"model_out\", model_out[:3])\n",
    "\n",
    "            # print(\"gt\", self.g(t))\n",
    "\n",
    "            # print(\"noise schedule\")\n",
    "            # print(self.noise_schedule.term1, self.noise_schedule.term2)\n",
    "\n",
    "            # print(\n",
    "            #     2\n",
    "            #     * noise_schedule.rho\n",
    "            #     * (noise_schedule.term1 + t * noise_schedule.term2)\n",
    "            #     ** (2 * noise_schedule.rho - 1)\n",
    "            #     * noise_schedule.term2\n",
    "            # )\n",
    "            # return\n",
    "            drift_A = torch.zeros(x.shape[0]).to(x.device)\n",
    "\n",
    "            # print(\"drift_X\", drift_X[:3])\n",
    "\n",
    "            if resampling_interval == -1:\n",
    "                return drift_X.detach(), drift_A.detach()\n",
    "\n",
    "            Ut = self.energy_net.forward_energy(t, x, beta)\n",
    "            # Ut = torch.vmap(true_Ut, in_dims=(None, 0, 0, None))(\n",
    "            #     target, t, x, self.noise_schedule\n",
    "            # )\n",
    "            if self.score_model is not None:\n",
    "                div_bt = compute_laplacian_true_exact(\n",
    "                    bt,\n",
    "                    x,\n",
    "                )\n",
    "            else:\n",
    "                laplacian_Ut = compute_laplacian_true_exact(\n",
    "                    nabla_Ut,\n",
    "                    x,\n",
    "                )\n",
    "                div_bt = -laplacian_Ut * (self.g(t).pow(2) / 2)\n",
    "\n",
    "            dUt_dt = torch.autograd.grad(Ut.sum(), t, create_graph=True)[0]\n",
    "\n",
    "            drift_A = (\n",
    "                self.gamma**2 * (-nabla_Ut * bt).sum(-1)\n",
    "                + self.gamma * div_bt\n",
    "                + self.gamma * dUt_dt\n",
    "            )\n",
    "\n",
    "        return drift_X.detach(), drift_A.detach()\n",
    "\n",
    "    def g(self, t):\n",
    "        g = self.noise_schedule.g(t)\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(outer_epochs)):\n",
    "    for epoch in range(inner_epochs):\n",
    "        break\n",
    "        total_loss = 0.0\n",
    "        for beta in betas:\n",
    "            for samples, _ in trainloader[beta]:\n",
    "                samples = samples.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                t = torch.rand(len(samples)).to(device)\n",
    "\n",
    "                loss = score_loss(energy_net, samples, t, noise_schedule, beta)\n",
    "                # loss = cfm_loss(model,\n",
    "                #                 samples,\n",
    "                #                 t,\n",
    "                #                 beta)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if epoch % 99 == 0:\n",
    "                print(\"Epoch %d | Loss %f\" % (epoch, loss.item()))\n",
    "\n",
    "    # Generate samples using the network\n",
    "    print(\"hi\")\n",
    "    ve_reverse_sde = VEReverseSDE(energy_net, noise_schedule, score_model=energy_net)\n",
    "    for beta in betas:\n",
    "        samples_not_resampled, _ = generate_samples_weighted(\n",
    "            ve_reverse_sde,\n",
    "            prior=priors[beta],\n",
    "            num_samples=1000,\n",
    "            t_span=(0, 1),\n",
    "            resampling_interval=-1,\n",
    "            inverse_temp=beta,\n",
    "            resample_at_end=False,\n",
    "        )\n",
    "        print(\"not_resampled data\")\n",
    "        target.get_dataset_fig(\n",
    "            target.unnormalize(samples_not_resampled),\n",
    "            title=f\"Temp {1/beta:.3f} Not Resampled\",\n",
    "        )\n",
    "\n",
    "        samples, _ = generate_samples_weighted(\n",
    "            ve_reverse_sde,\n",
    "            prior=priors[beta],\n",
    "            num_samples=1000,\n",
    "            t_span=(0, 1),\n",
    "            resampling_interval=1,\n",
    "            inverse_temp=beta,\n",
    "            resample_at_end=False,\n",
    "        )\n",
    "        print(\"resampled_data\")\n",
    "\n",
    "        _, log_weights = generate_samples_weighted(\n",
    "            ve_reverse_sde,\n",
    "            prior=priors[beta],\n",
    "            num_samples=1000,\n",
    "            t_span=(0, 1),\n",
    "            resampling_interval=2000,\n",
    "            inverse_temp=beta,\n",
    "            resample_at_end=False,\n",
    "        )\n",
    "\n",
    "        # new plot figure\n",
    "        fig = plt.figure()\n",
    "        plt.plot(torch.linspace(1, 0, 1000), log_weights.cpu().detach().numpy()[:, :40])\n",
    "        plt.xlabel(\"Integration Time\")\n",
    "        plt.ylabel(\"dlogW\")\n",
    "        plt.show()\n",
    "        samples_energy = target(samples)\n",
    "\n",
    "        # Update Buffer\n",
    "        buffer[beta].add(samples, samples_energy)\n",
    "\n",
    "        print(\"Mean of Energy\", samples_energy.mean())\n",
    "\n",
    "        samples_unnorm = target.unnormalize(samples)\n",
    "        plt.scatter(\n",
    "            samples_unnorm[:, 0].detach().cpu(),\n",
    "            samples_unnorm[:, 1].detach().cpu(),\n",
    "            label=\"Samples\",\n",
    "            alpha=0.5,\n",
    "            s=5,\n",
    "        )\n",
    "        plt.xlim(-50, 50)\n",
    "        plt.ylim(-50, 50)\n",
    "        plt.show()\n",
    "        target.get_dataset_fig(samples_unnorm, title=f\"Temp {1/beta:.3f}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Resample data from the buffer\n",
    "        samples, samples_energy, _ = buffer[beta].sample(512 * 10)\n",
    "        train_dataset = TensorDataset(samples, samples_energy)\n",
    "        # trainloader[beta] = DataLoader(train_dataset, batch_size = 512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
