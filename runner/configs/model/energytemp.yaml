_target_: src.models.energytemp_module.energyTempModule

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 0.001
  weight_decay: 0.0

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.1
  patience: 10

defaults:
  - net:
      - mlp
  - noise_schedule:
      - elucidating

partial_buffer:
  _target_: src.models.components.prioritised_replay_buffer.SimpleBuffer
  _partial_: true
  dim: ${energy.dimensionality}
  max_length: 10000
  min_sample_length: 1
  initial_sampler: null
  sample_with_replacement: True
  fill_buffer_during_init: False
  prioritize: False

partial_prior:
  _target_: src.energies.base_prior.MeanFreePrior
  _partial_: true
  n_particles: ${energy.n_particles}
  spatial_dim: ${energy.spatial_dim}

num_init_samples: 1024
num_samples_to_generate_per_epoch: 1024
num_samples_to_sample_from_buffer: 512

num_integration_steps: 1000

lr_scheduler_update_frequency: ${trainer.check_val_every_n_epoch}

# compile model for faster training with pytorch 2.0
compile: true

# initialize the buffer with samples from the prior
init_from_prior: false

num_samples_to_save: 100000

num_negative_time_steps: 0

# energytemp specific parameters
resampling_interval: -1
num_eval_samples: 1024
scale_diffusion: false
test_batch_size: 5000
start_resampling_step: 50

precondition_beta: false
train_on_all_temps: true
