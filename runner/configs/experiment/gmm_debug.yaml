# @package _global_

# to execute this experiment run:
# python train.py experiment=example

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["GMM"]

seed: 12345

logger:
  wandb:
    tags: ${tags}
    group: "gmm"

defaults:
  - override /energy: gmm
  - override /model/net: mlp_temp
  - override /model/noise_schedule: elucidating

model:
  partial_prior:
    _target_: src.energies.base_prior.Prior
    _partial_: true
    dim: 2

  # Elucidating noise schedule
  P_mean: -1.2
  P_std: 1.2

  diffusion_scale: 1.0
  # num_negative_time_steps: 0              # default
  num_samples_to_generate_per_epoch: 1024
  # num_samples_to_sample_from_buffer: 512  # default
  # num_samples_to_save: 10000    # default
  # num_integration_steps: 1000   # default

  buffer:
    prioritize: false

  clipper:
    _target_: src.models.components.clipper.Clipper
    should_clip_scores: True
    should_clip_log_rewards: False
    max_score_norm: 50
    min_log_reward: null

  lambda_weighter:
    _target_: src.models.components.lambda_weighter.BasicLambdaWeighter
    # _target_: src.models.components.lambda_weighter.NoLambdaWeighter
    _partial_: true
    epsilon: 1e-3
  
  optimizer:
    lr: 1e-3

  # this has to be max 1000 since test_set is 1000
  eval_batch_size: 1000
  # scheduler: null

  # energytemp parameters
  num_init_samples: 10000
  test_batch_size: 5000
  num_eval_samples: 2048        # 1024
  inference_batch_size: 512 
  resampling_interval: 1
  start_resampling_step: 0
  end_resampling_step: 900    
  scale_diffusion: False
  resample_at_end: True

  lower_temperature: 0.1      # currently not used
  higher_temperature: 1.0     # currently not used
  d_temp: 0.1                 # currently not used

  temperatures:               # currently used
    - 1.0
    
  loss_weights:
    energy_score: 1.0
    score: 1.0
    energy_matching: 1.0
    target_score: 0.0
    dem_energy: 0.0
  
  init_from_prior: false      # default

trainer:
  check_val_every_n_epoch: 5

